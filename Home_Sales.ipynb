{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_KW73O2e3dw",
        "outputId": "3fe010a9-d03e-4162-9dd6-4083e041d738"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425345 sha256=1f109792af59e4c4aa8df55336b0ab8fc3edb17bb373fad531c81508e09d0edc\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!wget -q https://dlcdn.apache.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.0-bin-hadoop3.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2XbWNf1Te5fM"
      },
      "outputs": [],
      "source": [
        "# Import packages\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
        "import time\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"id\", StringType(), True),\n",
        "    StructField(\"date\", StringType(), True),\n",
        "    StructField(\"date_built\", IntegerType(), True),\n",
        "    StructField(\"price\", IntegerType(), True),\n",
        "    StructField(\"bedrooms\", IntegerType(), True),\n",
        "    StructField(\"bathrooms\", IntegerType(), True),\n",
        "    StructField(\"sqft_living\", IntegerType(), True),\n",
        "    StructField(\"sqft_lot\", IntegerType(), True),\n",
        "    StructField(\"floors\", IntegerType(), True),\n",
        "    StructField(\"waterfront\", IntegerType(), True),\n",
        "    StructField(\"view\", IntegerType(), True),\n",
        "])\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wOJqxG_RPSwp"
      },
      "outputs": [],
      "source": [
        "# 1. Read in the AWS S3 bucket into a DataFrame.\n",
        "from pyspark import SparkFiles\n",
        "url = \"https://2u-data-curriculum-team.s3.amazonaws.com/dataviz-classroom/v1.2/22-big-data/home_sales_revised.csv\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RoljcJ7WPpnm"
      },
      "outputs": [],
      "source": [
        "# 2. Create a temporary view of the DataFrame.\n",
        "spark.sparkContext.addFile(url)\n",
        "file_path = \"file://\" + SparkFiles.get(\"home_sales_revised.csv\")\n",
        "\n",
        "df = spark.sql(f\"SELECT * FROM csv.`{file_path}`\")\n",
        "df.createOrReplaceTempView(\"home_sales\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxS7wrKI07K-",
        "outputId": "c2db4f45-cf13-4f36-c4b6-e898d08ce705"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- _c0: string (nullable = true)\n",
            " |-- _c1: string (nullable = true)\n",
            " |-- _c2: string (nullable = true)\n",
            " |-- _c3: string (nullable = true)\n",
            " |-- _c4: string (nullable = true)\n",
            " |-- _c5: string (nullable = true)\n",
            " |-- _c6: string (nullable = true)\n",
            " |-- _c7: string (nullable = true)\n",
            " |-- _c8: string (nullable = true)\n",
            " |-- _c9: string (nullable = true)\n",
            " |-- _c10: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.withColumnRenamed(\"_c0\", \"id\") \\\n",
        "    .withColumnRenamed(\"_c1\", \"date\") \\\n",
        "    .withColumnRenamed(\"_c2\", \"date_built\") \\\n",
        "    .withColumnRenamed(\"_c3\", \"price\") \\\n",
        "    .withColumnRenamed(\"_c4\", \"bedrooms\") \\\n",
        "    .withColumnRenamed(\"_c5\", \"bathrooms\") \\\n",
        "    .withColumnRenamed(\"_c6\", \"sqft_living\") \\\n",
        "    .withColumnRenamed(\"_c7\", \"sqft_lot\") \\\n",
        "    .withColumnRenamed(\"_c8\", \"floors\") \\\n",
        "    .withColumnRenamed(\"_c9\", \"waterfront\") \\\n",
        "    .withColumnRenamed(\"_c10\", \"view\")"
      ],
      "metadata": {
        "id": "pvQIQ7RO1yCD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.createOrReplaceTempView(\"home_sales\")"
      ],
      "metadata": {
        "id": "24iqf-Ki1zIB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPg9LlPX12Zw",
        "outputId": "e0234653-1f56-4865-ca4f-acf5e30aaa5d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: string (nullable = true)\n",
            " |-- date: string (nullable = true)\n",
            " |-- date_built: string (nullable = true)\n",
            " |-- price: string (nullable = true)\n",
            " |-- bedrooms: string (nullable = true)\n",
            " |-- bathrooms: string (nullable = true)\n",
            " |-- sqft_living: string (nullable = true)\n",
            " |-- sqft_lot: string (nullable = true)\n",
            " |-- floors: string (nullable = true)\n",
            " |-- waterfront: string (nullable = true)\n",
            " |-- view: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.types import FloatType, IntegerType, DateType\n",
        "\n",
        "df = df.withColumn(\"id\", col(\"id\").cast(FloatType()))\n",
        "df = df.withColumn(\"date\", col(\"date\").cast(DateType()))\n",
        "df = df.withColumn(\"date_built\", col(\"date_built\").cast(DateType()))\n",
        "df = df.withColumn(\"price\", col(\"price\").cast(IntegerType()))\n",
        "df = df.withColumn(\"bedrooms\", col(\"bedrooms\").cast(IntegerType()))\n",
        "df = df.withColumn(\"bathrooms\", col(\"bathrooms\").cast(IntegerType()))\n",
        "df = df.withColumn(\"sqft_living\", col(\"sqft_living\").cast(IntegerType()))\n",
        "df = df.withColumn(\"sqft_lot\", col(\"sqft_lot\").cast(IntegerType()))\n",
        "df = df.withColumn(\"floors\", col(\"floors\").cast(IntegerType()))\n",
        "df = df.withColumn(\"waterfront\", col(\"waterfront\").cast(IntegerType()))\n",
        "df = df.withColumn(\"view\", col(\"view\").cast(IntegerType()))"
      ],
      "metadata": {
        "id": "fmVCzJhw2gCJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.createOrReplaceTempView(\"home_sales\")"
      ],
      "metadata": {
        "id": "zn05AQpF2txm"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRIR8Nke22H3",
        "outputId": "96301314-de3d-482f-8ad6-d3e0a6b64074"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: float (nullable = true)\n",
            " |-- date: date (nullable = true)\n",
            " |-- date_built: date (nullable = true)\n",
            " |-- price: integer (nullable = true)\n",
            " |-- bedrooms: integer (nullable = true)\n",
            " |-- bathrooms: integer (nullable = true)\n",
            " |-- sqft_living: integer (nullable = true)\n",
            " |-- sqft_lot: integer (nullable = true)\n",
            " |-- floors: integer (nullable = true)\n",
            " |-- waterfront: integer (nullable = true)\n",
            " |-- view: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "L6fkwOeOmqvq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab9d00bf-c30e-49b9-9ef1-b9e15ddf54ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------------+\n",
            "|      year|         avg_price|\n",
            "+----------+------------------+\n",
            "|2010-01-01| 296800.7544776119|\n",
            "|2011-01-01|302141.89581749047|\n",
            "|2012-01-01| 298233.4150805271|\n",
            "|2013-01-01|299999.38822652755|\n",
            "|2014-01-01| 299073.8858447489|\n",
            "|2015-01-01|307908.86020761245|\n",
            "|2016-01-01|296050.24326347304|\n",
            "|2017-01-01| 296576.6934782609|\n",
            "+----------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 3. What is the average price for a four bedroom house sold in each year rounded to two decimal places?\n",
        "question_3 = \"\"\"\n",
        "SELECT date_built as year, AVG(price) AS avg_price\n",
        "FROM home_sales\n",
        "WHERE bedrooms = 4\n",
        "GROUP BY date_built\n",
        "ORDER BY date_built\n",
        "\"\"\"\n",
        "\n",
        "result_3 = spark.sql(question_3)\n",
        "result_3.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "l8p_tUS8h8it",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7399d38-66f6-40fa-ffc3-4f8fecb9b13d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------------+\n",
            "|year_built|         avg_price|\n",
            "+----------+------------------+\n",
            "|2010-01-01|  292859.615942029|\n",
            "|2011-01-01|291117.46706586826|\n",
            "|2012-01-01| 293683.1872074883|\n",
            "|2013-01-01|295962.27145085804|\n",
            "|2014-01-01| 290852.2661870504|\n",
            "|2015-01-01| 288770.2966101695|\n",
            "|2016-01-01|  290555.073964497|\n",
            "|2017-01-01| 292676.7887740029|\n",
            "+----------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 4. What is the average price of a home for each year the home was built that have 3 bedrooms and 3 bathrooms rounded to two decimal places?\n",
        "question_4 = \"\"\"\n",
        "SELECT date_built as year_built, AVG(price) AS avg_price\n",
        "FROM home_sales\n",
        "WHERE bedrooms = 3 AND bathrooms = 3\n",
        "GROUP BY date_built\n",
        "ORDER BY date_built\n",
        "\"\"\"\n",
        "\n",
        "result_4 = spark.sql(question_4)\n",
        "result_4.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Y-Eytz64liDU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a437e4b5-e34f-4a85-a282-69d93b3f6f6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------------+\n",
            "|year_built|         avg_price|\n",
            "+----------+------------------+\n",
            "|2010-01-01| 285010.2215909091|\n",
            "|2011-01-01| 276553.8128654971|\n",
            "|2012-01-01|307539.97402597405|\n",
            "|2013-01-01|      303676.79375|\n",
            "|2014-01-01| 298264.7183908046|\n",
            "|2015-01-01| 297609.9679144385|\n",
            "|2016-01-01| 293965.1046511628|\n",
            "|2017-01-01|280317.57692307694|\n",
            "+----------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 5. What is the average price of a home for each year built that have 3 bedrooms, 3 bathrooms, with two floors,\n",
        "# and are greater than or equal to 2,000 square feet rounded to two decimal places?\n",
        "question_5 = \"\"\"\n",
        "SELECT date_built as year_built, AVG(price) AS avg_price\n",
        "FROM home_sales\n",
        "WHERE bedrooms = 3 AND bathrooms = 3 AND floors = 2 AND sqft_living >= 2000\n",
        "GROUP BY date_built\n",
        "ORDER BY date_built\n",
        "\"\"\"\n",
        "\n",
        "result_5 = spark.sql(question_5)\n",
        "result_5.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUrfgOX1pCRd",
        "outputId": "0945f61d-772f-4241-d0e5-df5e23ca24a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+------------------+\n",
            "|view|         avg_price|\n",
            "+----+------------------+\n",
            "|   0|276161.10714285716|\n",
            "|   1|         280277.25|\n",
            "|   2|         254207.04|\n",
            "|   3|258428.27777777778|\n",
            "|   4| 287105.1724137931|\n",
            "|   5|286016.53571428574|\n",
            "|   6|         323486.72|\n",
            "|   7|271921.04761904763|\n",
            "|   8|        298208.875|\n",
            "|   9| 290356.6923076923|\n",
            "|  10|          303305.4|\n",
            "|  11|274758.85714285716|\n",
            "|  12|280996.63636363635|\n",
            "|  13|291548.53846153844|\n",
            "|  14|         318510.96|\n",
            "|  15| 281520.1515151515|\n",
            "|  16|290491.17647058825|\n",
            "|  17| 287555.9166666667|\n",
            "|  18|293643.92307692306|\n",
            "|  19| 277739.1935483871|\n",
            "+----+------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "--- 1.049471139907837 seconds ---\n"
          ]
        }
      ],
      "source": [
        "# 6. What is the \"view\" rating for the average price of a home, rounded to two decimal places, where the homes are greater than\n",
        "# or equal to $350,000? Although this is a small dataset, determine the run time for this query.\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "question_6 = \"\"\"\n",
        "SELECT view, AVG(price) AS avg_price\n",
        "FROM home_sales\n",
        "WHERE bedrooms = 3 AND bathrooms = 3 AND floors = 2 AND sqft_living >= 2000\n",
        "GROUP BY view\n",
        "ORDER BY view\n",
        "\"\"\"\n",
        "\n",
        "result_6 = spark.sql(question_6)\n",
        "result_6.show()\n",
        "\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "KAhk3ZD2tFy8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3fc8eb7-38f0-4277-c63f-5a4cc4619266"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# 7. Cache the the temporary table home_sales.\n",
        "spark.sql(\"cache table home_sales\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "4opVhbvxtL-i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41ca5aa6-a466-4643-e538-32f4621d40c4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# 8. Check if the table is cached.\n",
        "spark.catalog.isCached('home_sales')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GnL46lwTSEk",
        "outputId": "19aa6602-4332-4d86-e164-2943ee305c38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+------------------+\n",
            "|view|         avg_price|\n",
            "+----+------------------+\n",
            "|   0|403848.51461988303|\n",
            "|   1| 401044.2513368984|\n",
            "|   2|397389.24752475246|\n",
            "|   3| 398867.5964912281|\n",
            "|   4|         399631.89|\n",
            "|   5| 401471.8248587571|\n",
            "|   6| 395655.3789473684|\n",
            "|   7| 403005.7709497207|\n",
            "|   8|398592.70658682636|\n",
            "|   9| 401393.3370786517|\n",
            "|  10|401868.42523364484|\n",
            "|  11|399548.11891891895|\n",
            "|  12| 401501.3243243243|\n",
            "|  13|         398917.98|\n",
            "|  14|398570.02923976607|\n",
            "|  15|404673.29545454547|\n",
            "|  16| 399586.5311004785|\n",
            "|  17|398474.49029126216|\n",
            "|  18| 399332.9090909091|\n",
            "|  19| 398953.1703296703|\n",
            "+----+------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "--- 0.4988260269165039 seconds ---\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# 9. Using the cached data, run the query that filters out the view ratings with average price\n",
        "#  greater than or equal to $350,000. Determine the runtime and compare it to uncached runtime.\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "question_9_cached = \"\"\"SELECT view, AVG(price) AS avg_price\n",
        "                      FROM home_sales\n",
        "                      WHERE price >= 350000\n",
        "                      GROUP BY view\n",
        "                      ORDER BY view\"\"\"\n",
        "\n",
        "result_9_cached = spark.sql(question_9_cached)\n",
        "result_9_cached.show()\n",
        "\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "spark.sql(\"UNCACHE TABLE home_sales\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Qm12WN9isHBR"
      },
      "outputs": [],
      "source": [
        "# 10. Partition by the \"date_built\" field on the formatted parquet home sales data\n",
        "df.write.partitionBy(\"date_built\").parquet(\"/content/parquet_home_sales\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "AZ7BgY61sRqY"
      },
      "outputs": [],
      "source": [
        "# 11. Read the parquet formatted data.\n",
        "parquet_df = spark.read.parquet(\"/content/parquet_home_sales\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "J6MJkHfvVcvh"
      },
      "outputs": [],
      "source": [
        "# 12. Create a temporary table for the parquet data.\n",
        "parquet_df.createOrReplaceTempView(\"parquet_home_sales\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_Vhb52rU1Sn",
        "outputId": "44aa76ed-4648-4a98-cf8b-939c6621bde7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+------------------+\n",
            "|view|         avg_price|\n",
            "+----+------------------+\n",
            "|   0|403848.51461988303|\n",
            "|   1| 401044.2513368984|\n",
            "|   2|397389.24752475246|\n",
            "|   3| 398867.5964912281|\n",
            "|   4|         399631.89|\n",
            "|   5| 401471.8248587571|\n",
            "|   6| 395655.3789473684|\n",
            "|   7| 403005.7709497207|\n",
            "|   8|398592.70658682636|\n",
            "|   9| 401393.3370786517|\n",
            "|  10|401868.42523364484|\n",
            "|  11|399548.11891891895|\n",
            "|  12| 401501.3243243243|\n",
            "|  13|         398917.98|\n",
            "|  14|398570.02923976607|\n",
            "|  15|404673.29545454547|\n",
            "|  16| 399586.5311004785|\n",
            "|  17|398474.49029126216|\n",
            "|  18| 399332.9090909091|\n",
            "|  19| 398953.1703296703|\n",
            "+----+------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "--- 0.6894721984863281 seconds ---\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "# 13. Run the query that filters out the view ratings with average price of greater than or equal to $350,000\n",
        "# with the parquet DataFrame. Round your average to two decimal places.\n",
        "# Determine the runtime and compare it to the cached version.\n",
        "\n",
        "spark.sql(\"CACHE TABLE parquet_home_sales\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "question_13 = \"\"\"SELECT view, AVG(price) AS avg_price\n",
        "                 FROM parquet_home_sales\n",
        "                 WHERE price >= 350000\n",
        "                 GROUP BY view\n",
        "                 ORDER BY view\"\"\"\n",
        "\n",
        "result_13 = spark.sql(question_13)\n",
        "result_13.show()\n",
        "\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "# Uncache the parquet DataFrame after the query is executed\n",
        "spark.sql(\"UNCACHE TABLE parquet_home_sales\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "hjjYzQGjtbq8"
      },
      "outputs": [],
      "source": [
        "# 14. Uncache the home_sales temporary table.\n",
        "spark.catalog.uncacheTable(\"home_sales\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Sy9NBvO7tlmm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae54ff2b-694c-40c1-9208-6f3c01cb0e72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is home_sales cached? No\n"
          ]
        }
      ],
      "source": [
        "# 15. Check if the home_sales is no longer cached\n",
        "print(\"Is home_sales cached?\", \"Yes\" if spark.catalog.isCached(\"home_sales\") else \"No\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "nteract": {
      "version": "0.28.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}